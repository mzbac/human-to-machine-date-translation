{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "CSV_PATH = 'human-machine.csv'\n",
    "\n",
    "class CharDict:\n",
    "    def __init__(self):\n",
    "        self.char2index = {}\n",
    "        self.char2count = {}\n",
    "        self.index2char = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_chars = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for c in sentence:\n",
    "            self.addChar(c)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    human = df[\"human\"].values\n",
    "    machine = df[\"machine\"].values\n",
    "    pairs = []\n",
    "    for i in range(len(human)):\n",
    "        pairs.append([human[i],machine[i]])\n",
    "        \n",
    "    input_lang = CharDict()\n",
    "    output_lang = CharDict()\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.char2index[char] for char in sentence]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        \n",
    "        # Combine embedded input word and last context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip = 5.0\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "\n",
    "        # Get most likely word index (highest value) from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]])) # Chosen word is next input\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "        # Stop at end of sentence (not necessary when using known targets)\n",
    "        if ni == EOS_token: break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_model = 'general'\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_chars, hidden_size, n_layers)\n",
    "decoder = AttnDecoderRNN(attn_model, hidden_size, output_lang.n_chars, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 2600\n",
    "plot_every = 100\n",
    "print_every = 200\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anchen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/anchen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 13s (- 14m 41s) (200 7%) 1.6692\n",
      "2m 25s (- 13m 22s) (400 15%) 1.1330\n",
      "3m 40s (- 12m 16s) (600 23%) 0.5032\n",
      "4m 59s (- 11m 14s) (800 30%) 0.3321\n",
      "6m 19s (- 10m 7s) (1000 38%) 0.2221\n",
      "7m 41s (- 8m 58s) (1200 46%) 0.2167\n",
      "9m 5s (- 7m 47s) (1400 53%) 0.1544\n",
      "10m 28s (- 6m 32s) (1600 61%) 0.1804\n",
      "11m 53s (- 5m 16s) (1800 69%) 0.1411\n",
      "13m 17s (- 3m 59s) (2000 76%) 0.1186\n",
      "14m 42s (- 2m 40s) (2200 84%) 0.1103\n",
      "16m 8s (- 1m 20s) (2400 92%) 0.0971\n",
      "17m 33s (- 0m 0s) (2600 100%) 0.0590\n"
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    training_pair = variables_from_pair(random.choice(pairs))\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20d4eaff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQv8HXF86UpAC2DQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20cc96f3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=11):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2char[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words = evaluate(pair[0])\n",
    "    output_sentence = ''.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 17 apr 1980\n",
      "= 1980-04-17\n",
      "< 1980-04-17<EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anchen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/anchen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
